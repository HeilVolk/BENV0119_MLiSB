{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3 - KNN and SVR: Hyper-parameter tuning and model testing\n",
    "In this third tutorial, we will look at two Machine Learning algorithms, K-Nearest Neighbours (KNN) and Support Vector Regression (SVR). For these algorithms, we will look at three aspects:\n",
    "1. **Effect of hyper-parameters**: First, we will look into the effect of different hyper-parameters on the model performance, especially with regards to over-fitting and under-fitting.\n",
    "2. **Hyper-parameter tuning using cross-validation**: We will look at some systematic approaches to optimize the hyper-parameters of KNN and SVR. \n",
    "3. **Model testing**: After hyper-parameter tuning, we should always test the model's performance on an independent test set *which was not used for tuning*. The error metrics obtained from the test set are used to report a model's performance and to compare different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Load and preprocess data\n",
    "Both the KNN and the SVR algorithm perform best if their features are scaled. We can hence load the scaled and shuffled dataset we have created in Tutorial 2, as it contains all pre-processing steps we have previously performed. During preprocessing, we will also split the data into features and targets, select the k best features (see Tutorial 2) and divide into training and validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Import packages and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we shall always start by loading necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages to read the data\n",
    "import pandas as pd\n",
    "import numpy  as np # numpy is always useful\n",
    "import os           # Loads the functions related to the operating system \n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection   import train_test_split\n",
    "\n",
    "from lib_file import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = # TASK: Load the data from 'training_scaled.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Split features and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data, we should extract the features and targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_name   = 'tilted_radiation'\n",
    "features_list = [ 'roof_tilt', 'roof_aspect', 'roof_area', 'roof_x', 'roof_y', 'aspect_sign', 'SIS',\n",
    "                  'SISDIR', 'DHI', 'horizon_S', 'horizon_SSW', 'horizon_SWW', 'horizon_W',\n",
    "                  'horizon_NWW', 'horizon_SSE', 'horizon_SEE', 'horizon_E', 'horizon_NEE',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = ## TASK: EXTRACT THE TARGET COLUMN FROM data\n",
    "X = ## TASK: EXTRACT THE FEATURES FROM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3: Extract optimal number of features\n",
    "In Tutorial 2, we have assessed the **optimal number of features** for modelling using a default KNN.\n",
    "As the results in Tutorial 2 vary slightly due to the re-shuffling of the data, we will define it here as **6** for consistency. A set of 6 features should always yield some of the best results, even if not the very best.\n",
    "\n",
    "**NOTE**: The optimal number of features can vary for different algorithms and hyper-parameter configurations. However, for the given exercise we will assume that the feature selection using the default KNN is representative for both KNN and SVR algorithms. For our dataset, this is approximately true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_features = 6   # Define the optimal number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we extract the variable `X_tuning` that contains the reduced dataset with only 6 features (based on the mutual regression criterion):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mutual_info = ## TASK: INITIALIZE THE MUTUAL_INFO SELECTOR WITH n_features\n",
    "X_tuning    = ## TASK: EXTRACT THE BEST FEATURES FROM X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Question*\n",
    "- Which are the selectede best 6 features? (*HINT*: refer to Tutorial 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4: Split data into training and validation\n",
    "For the initial exploration of the model parameters, we are mostly interested to look into *over-fitting* and *under-fitting* behaviour. For this, it is essential that we compare the *training* and *validation*-scores as we have done in Tutorial 1. We hence split the data into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = # TASK: Split the data into 80% training and 20% validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: For cross-validation, we will not use the training and validation sets, but instead we use directly the `X_tuning` and `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN regression algorithm is imported from the `sklean.neighbours` module and is documented here:<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Questions*:\n",
    "- Which are the main hyper-parameters of KNN and what are their variable names in `scikit-learn`?\n",
    "- What are the default values of these hyper-parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Hyper-Parameter analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, the default hyper-parameters are not those that provide the best results. To find the optimal hyper-parameters for a given dataset, we must **tune** the model. To develop a good intuition for the parameter choices during tuning, we need to understand *how* these impact the model performance. This is best assessed by comparing training and validation scores, which unveil some information about under-fitting and over-fitting behaviour. Hence, in this Step we will use the training and validation sets obtained in Step 1.4.\n",
    "\n",
    "Here, we will focus on the most important hyper-parameter of KNN, the number of neighbours (`n_neighbors`, let's call it *k*), and its impact on the model performance. \n",
    "In Step 2.2, we will look at the systematic tuning of the algorithm and other hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When tuning an ML model, it is always useful to keep the default performance in mind. We can obtain the default performance as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor() # Initiate model\n",
    "knn.fit(X_train, y_train)   # Fit model with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( 'Training score: %.3f' %knn.score(X_train, y_train) )\n",
    "print( 'Validation score: %.3f' %knn.score(X_val, y_val) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_What if n_neighbours is very small?_**\n",
    "\n",
    "The minimum *k* that we can choose is 1. So, let's train and score a KNN with 1 neighbor:\n",
    "\n",
    "*HINT*: Hyper-parameters are always set at the *initialisation* of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn = ## TASK: INITIATE MODEL WITH 1 NEIGHBOR\n",
    "## TASK: FIT THE MODEL WITH THE TRAINING DATA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And assess the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TASK: COMPUTE AND DISPLAY THE TRAINING SCORE\n",
    "## TASK: COMPUTE AND DISPLAY THE VALIDATION SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_What if n_neighbours is very large?_**\n",
    "\n",
    "The other extreme choice of *k* is a very large value. The maximum possible value is equal to the length of the training data (800 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn = ## TASK: INITIATE MODEL WITH 800 NEIGHBORS\n",
    "## TASK: FIT THE MODEL WITH THE TRAINING DATA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TASK: COMPUTE AND DISPLAY THE TRAINING SCORE\n",
    "## TASK: COMPUTE AND DISPLAY THE VALIDATION SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Questions*\n",
    "- a. Why is the KNN *training* score with 1 neighbour always 1, and with 800 neighbours always 0?\n",
    "- b. How does the *validation* score compare to the *training* score for *k = 1* and *k = 800*?\n",
    "- c. In which case is the model over-fitting, and where is it under-fitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: Hyper-Parameter tuning\n",
    "Testing every possible hyper-parameter individually, as we have done above, is clearly very time-consuming. To facilitate the process, we will use the `GridSearchCV` function of `scikit-learn`. This function allows to perform cross-validation for a defined set of values of one or several hyper-parameters, which has several advantages:\n",
    "\n",
    "- It avoids the use of long codes for tuning (e.g. nested for-loops for tuning multiple parameters)\n",
    "- It automatically stores all the cross-validation scores and computes the statistics\n",
    "- It also logs the fitting and scoring times, which gives insight in the trade-off between computational time and performance\n",
    "- It can run *in parallel*, which reduces the required time for testing large amounts of hyper-parameters\n",
    "- It re-fits the estimator with the best performance on the entire training dataset, which can be used for testing (see Step 2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time     # For logging execution times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GridSearchCV` requires as input a default estimator and a dictionary with the hyper-parameter values to be tested. In our case, we have only one main parameter (`n_neighbors`). We can hence define a range of values for which to tune the estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparams = { 'n_neighbors' : np.arange(1, 50) }  # Test values between 1 and 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearchCV is performed by initialising a default model, declaring the GridSearchCV instance and fitting it on the *entire* feature set (performing the CV grid search). Additionally, we can log the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tt = time.time()\n",
    "\n",
    "knn = KNeighborsRegressor()                        # Initiate default KNN\n",
    "knn_gridsearch = GridSearchCV(knn, hyperparams)    # Declare GridSearch instance\n",
    "knn_gridsearch.fit(X_tuning, y)                    # Fit the grid-search (i.e. perform the grid-search)\n",
    "\n",
    "print('Performed hyperparameter tuning in %.2fs' %(time.time()-tt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results of GridSearchCV\n",
    "There are plenty of results produced by the GridSearchCV, but we are interested in three main ones:\n",
    "- The average validation score across all folds\n",
    "- The fitting time\n",
    "- The scoring time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are saved in the `cv_results_` attribute of the gridsearch instance. We can visualise them by loading them into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_gridsearch_results = pd.DataFrame(knn_gridsearch.cv_results_)\n",
    "knn_gridsearch_results.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the score changes with `n_neighbors`, we can look at the `mean_test_score` and the `std_test_score`, which shows the average validation score and its standard deviation across all k folds. The value of `n_neighbors` corresponding to each row is stored in the column `param_n_neighbors`. This data can be easily visualised, just as the number of features from Tutorial 2 (Step 3.2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,4))\n",
    "knn_gridsearch_results.plot(x = 'param_n_neighbors', \n",
    "                            y = 'mean_test_score', \n",
    "                            yerr = 'std_test_score', \n",
    "                            ax = plt.gca())\n",
    "plt.ylabel('Cross-validation score ($R^2$)')\n",
    "plt.grid()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the **best parameters** and the **best score** are saved as separate attributes of the grid search instance, which can be used to train the optimal model. Let's show these two attributes:\n",
    "\n",
    "**NOTE**: The names of the attributes can be found in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TASK: Show the best parameters of the GridSearchCV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TASK: how the best score of the GridSearchCV results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Questions*\n",
    "- a. What is the optimal number of neighbours for this problem? What is the best performance that can be achieved?\n",
    "- b. The standard deviation is quite high. How can the standard deviation be reduced?\n",
    "- c. [OPTIONAL]: What about other hyper-parameters? Does changing these (e.g. `weights`, `metric`) improve the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3: Model testing\n",
    "Once we have completed the hyper-parameter tuning, we should always test the model performance on an independent **test set**. It is hereby important that the test data has not been used in the cross-validation process. In our example, the test data is taken from a different geographic area (see the Tutorial introduction document).\n",
    "The test data is used for the final model evaluation and for comparison to other models, since it is not biased by the tuning process.\n",
    "\n",
    "We have prepared a test dataset for you, which consists of two files:\n",
    "- *test_data.csv*: The test dataset, where the feature transformation from Tutorial 2 has been applied\n",
    "- *test_data_scaled.csv*: The above dataset, which has been standardized with respect to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and transform test data\n",
    "Since in this tutorial we work with scaled data, we should load the scaled test data and extract the features and target similar to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data_scaled.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test = test_data[ features_list ]\n",
    "y_test = test_data[ target_name ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we also applied the mutual information criterion above to select only the 6 best features. We can use the same `SelectKBest` instance (called `mutual_info` in Step 3.3, and which is already fitted to the training data) to transform the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_reduced = mutual_info.transform( X_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict test labels\n",
    "Now, we should predict the test labels for the best hyper-parameters found in the model tuning process. For this, the `GridSearchCV` has a very useful functionality, as it re-fits an estimator with the best found parameters. To obtain a prediction (or a score), we hence have to simply call the `predict()` function as we usually do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = knn_gridsearch.predict( X_test_reduced )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate the test prediction\n",
    "To assess the test prediction, we can call the knn `score`-function, which we have also used during model tuning. However, as we have seen in Tutorial 1, the default $R^2$-score is not the only metric available to assess model performance. Usually, we try to compare different error metrics, in order to obtain a holistic overview of a model's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Compute the $R^2$-coefficient of determination and the MAE (mean absolute error) for the test set (refer to Tutorial 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualise the test prediction\n",
    "Finally, we can also visualise the test results in the form of a scatterplot, with the \"True\" labels on the x-axis, and the \"Predicted\" labels on the y-axis. Ideally, the points should lie on a straight line of form y=x (this would correspond to an R2-value of 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Create a scatterplot of the test samples, with the targets on the x-axis and the predictions on the y-axis. Label the axes correctly and and a line of y=x to indicate the \"perfect\" prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Questions*\n",
    "- a. How does the test score compare to the validation score?\n",
    "- b. How does the MAE compare to the MAE found for the linear regression in Tutorial 1?\n",
    "- c. Do you notice any systematic bias in your scatterplot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVR algorithm is imported from the `sklean.svm` module and is documented here:<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Questions*:\n",
    "- Which are the main hyper-parameters of SVM and what are their variable names in `scikit-learn`?\n",
    "- What are the default values of these hyper-parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Hyper-parameter analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the KNN, let's fit an SVR with default parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: initialise and fit a default SVR model and obtain the training and validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores of the default SVR are not satisfactory, so we need to change the hyper-parameters. Here, we will look at two parameters - `C` and `gamma`, while keeping the default kernel and $\\epsilon$. \n",
    "\n",
    "##### Varying C\n",
    "Let's start by changing `C` - either to a very small value (0.001) or to a very big value (say, 10,000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: initialise and fit two SVR models (one with `C = 0.001` and one with `C = 10000`) and obtain the training and validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Varying gamma\n",
    "To test the performance of `gamma`, we should first understand the default value. Based on the documentation, we can compute `gamma` as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gamma_default = 1 / (n_features * X_train.var())\n",
    "gamma_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again check some very small (0.00001) and very high (10) values. As the default model performs badly, let's choose a value of `C` that lies somewhere in the expected range (e.g. C = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: initialise and fit two SVR models (one with `gamma = 0.0001` and one with `gamma = 1`, and both with `C = 1000`) and obtain the training and validation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Questions*\n",
    "- a. How does the default model perform compared to the default KNN?\n",
    "- b. For which values of `C` does the model over-fit, when does it under-fit?\n",
    "- c. In which range do you expect the optimal value for `C`?\n",
    "- d. In which \"direction\" (increasing, decreasing) does `gamma` lead to overfitting, when does it underfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the hyper-parameters for the SVR is somewhat more cumbersome than for the KNN, for two reasons: \n",
    "1. We now have two main hyper-paramters to tune (`C` and `gamma`)\n",
    "2. The model training is slower for the SVR, so the GridSearch requires more computational time.\n",
    "\n",
    "To avoid searching a huge number of possible combinations, we can perform the GridSearchCV in an iterative fashion. First, we choose large ranges, which we decrease once we have identified the area where the model performs best to close in on the optimal values.\n",
    "\n",
    "Let's start by choosing hyper-parameters in the orders of magnitude around the default values (remember that we already have some knowledge of the approximate range of the hyper-parameters from Step 3.1!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'C'     : [1, 10, 100, 1000, 10000],      # Get orders of magnitude for C\n",
    "    'gamma' : [0.0001, 0.001, 0.01, 0.1, 1]   # Get orders of magnitude for gamma\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**:  If you have a strong machine and have a lot of time, you can run many parameters at once, but often it is good to \"test\" first for the correct range of hyper-parameters. If you feel that the hyper-parameter tuning takes an excessive amount of time, consider stopping the notebook and reducing the range of the tested values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dictionary of hyper-parameters and after declaring an instance of the default SVR model, we can call the `GridSearchCV` function and fit it with the features and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tt = time.time() # Log the current time\n",
    "\n",
    "# TASK: Declare default SVR     \n",
    "svr_gridsearch = # TASK: Declare an SVR grid-search with the hyperparameter dictionary\n",
    "# TASK: Fit the SVR grid-search (i.e. perform the grid-search over all hyper-parameters)\n",
    "\n",
    "print('Performed hyperparameter tuning in %.2fs' %(time.time()-tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svr_gridsearch_results = # TASK: Obtain cv_results_ as a dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the error changes with `C` and `gamma`, we can extract the `mean_test_score`. The data is best understandable if we expand it to two dimensions, one for C and one for gamma. For this, we can use panda's `pivot()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_val_score = svr_gridsearch_results.pivot(values = 'mean_test_score',   # Values to popoulate the 2D datagrame\n",
    "                                              index = 'param_C',            # New index\n",
    "                                              columns = 'param_gamma')      # New columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can then display as a formatted matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "format_df( mean_val_score, cmap='YlGn', decimals = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the time for fitting and scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_fit_time = ## TASK: PIVOT THE svr_gridsearch_results TO EXTRACT THE mean_fit_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "format_df( mean_fit_time, cmap='autumn_r', decimals = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_score_time = ## TASK: PIVOT THE svr_gridsearch_results TO EXTRACT THE mean_score_time\n",
    "## TASK: DISPLAY FORMATTED MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Go back to the beginning of Step 3.2 and change the hyper-parameter values, closing in on the range of values with the highest scores. Iterate this process until you have found a satisfactory best estimator.\n",
    "\n",
    "*HINT*: Don't forget about the *best parameter* and *best score* attributes that we have seen in Step 2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Questions*\n",
    "- a. In which range do you find the best hyper-parameters?\n",
    "- b. Which process (fitting or scoring) is more time-intensive for the SVR?\n",
    "- c. How do the fitting times compare to the performance? Do you notice a pattern? What does this suggest about the over/underfitting of the model?\n",
    "- [OPTIONAL] d. Expand the model tuning by other hyper-parameters. What is the best score that you can achieve? What are the hyper-parameters to achieve this score? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3: Evaluate the test set\n",
    "Similar to the KNN, we should evaluate the prediction on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_pred_svr = # TASK: Predict the test labels for the best estimator found in step 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Similar to Step 2.3, compute the R2 and MAE scores for the SVR on the test set, and create a scatterplot that shows both test predictions (KNN and SVR), as well as the line \"y = x\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Questions*\n",
    "- a. How does the test performance of the SVR compare to test performance of the KNN (R2 and MAE)?\n",
    "- b. How to the scatterplots of the two test predictions compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
